# Gradient Descent Visualization

## ğŸ“Œ Overview
This project demonstrates **Gradient Descent Optimization** using Python and NumPy. The function to be minimized is:

\[
f(x) = x^2 + 10 \sin(x)
\]

A **gradient descent algorithm** iteratively updates \( x \) based on the gradient of \( f(x) \) to find the minimum. The process is visualized using Matplotlib.

---

## ğŸ“Œ How Gradient Descent Works
Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of the **negative gradient**.

### **Gradient Descent Formula**
\[
x_{new} = x_{old} - \eta \cdot 
abla f(x)
\]
Where:
- \( \eta \) (**learning rate**) controls step size.
- \( 
abla f(x) \) is the gradient (derivative) of \( f(x) \).
- The process repeats for a set number of iterations or until convergence.

### **Gradient of the Function**
\[

abla f(x) = 2x + 10 \cos(x)
\]
This derivative is used in the update step of gradient descent.

---

## ğŸ“Œ Code Explanation

### **1ï¸âƒ£ Function Definition**
```python
def f(x):
    return x**2 + 10 * np.sin(x)
```
This function \( f(x) \) is the objective function to be **minimized**.

### **2ï¸âƒ£ Gradient (Derivative) of f(x)**
```python
def grad_f(x):
    return 2 * x + 10 * np.cos(x)
```
This computes the gradient (slope) of \( f(x) \), which helps in updating \( x \).

### **3ï¸âƒ£ Gradient Descent Algorithm**
```python
def gradient_descent(starting_point, learning_rate, num_iterations):
    x = starting_point
    history = [x]  # Store history of x values
    for _ in range(num_iterations):
        gradient = grad_f(x)
        x = x - learning_rate * gradient
        history.append(x)
    return x, history
```
- Starts at `starting_point`.
- Iteratively updates `x` using the **gradient descent formula**.
- Stores all values of `x` for visualization.

### **4ï¸âƒ£ Running Gradient Descent**
```python
starting_point = 0.0
learning_rate = 0.1
num_iterations = 50
x_opt, history = gradient_descent(starting_point, learning_rate, num_iterations)
```
- Starts at \( x = 0.0 \).
- Uses a **learning rate of 0.1**.
- Runs for **50 iterations**.

### **5ï¸âƒ£ Visualization**
```python
x_vals = np.linspace(-10, 10, 100)
plt.plot(x_vals, f(x_vals), label="f(x)")
plt.scatter(history, [f(x) for x in history], c="red", label="Gradient Descent Steps")
plt.legend()
plt.title("Gradient Descent Optimization")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.show()
```
- Plots \( f(x) \) as a curve.
- **Marks gradient descent steps in red**, showing how \( x \) moves toward the minimum.

---

## ğŸ“Œ Visualization Example
![Gradient Descent Visualization](gradient_descent_visualization.png)

The output plot shows:
- The **blue curve** as \( f(x) \).
- **Red dots** marking each step of gradient descent, moving toward the minimum.

---

## ğŸ“Œ How to Run the Code
### **1ï¸âƒ£ Install Required Libraries**
```bash
pip install numpy matplotlib
```
### **2ï¸âƒ£ Run the Python Script**
```bash
python gradient_descent.py
```

---

## ğŸ“Œ Parameters & Tuning
| Parameter | Description | Default |
|-----------|-------------|---------|
| `starting_point` | Initial value of \( x \) | `0.0` |
| `learning_rate` | Step size for gradient updates | `0.1` |
| `num_iterations` | Number of gradient descent steps | `50` |

#### **ğŸ”¹ Adjust Learning Rate**
- **Too small** â†’ Slow convergence.
- **Too large** â†’ Might overshoot and never converge.

---

## ğŸ“Œ Key Takeaways
âœ… **Fully Vectorized Gradient Descent**  
âœ… **Step-by-Step Visualization of Optimization**  
âœ… **Adjustable Learning Rate & Iterations**  

This project is a great way to understand how **gradient descent** optimizes functions in **machine learning**.

---

### **ğŸ“Œ Next Steps**
ğŸ”¹ Extend this to **higher-dimensional functions**  
ğŸ”¹ Implement **Momentum-based Gradient Descent**  
ğŸ”¹ Try **Adam Optimizer** for advanced optimization  

ğŸš€ **Happy Learning!**
